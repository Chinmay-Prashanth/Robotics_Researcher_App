# T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models

**Authors:** Yiteng Chen, Wenbo Li, Shiyi Wang, Huiping Zhuang, Qingyao Wu

**Published:** 2025-06-24

**arXiv URL:** http://arxiv.org/abs/2506.19498v1

**Categories:** cs.RO, cs.AI, I.2.9; I.2.10; I.4.8; H.5.2

**Pages:** 28

**PDF Status:** Not encrypted

## [LIST] Abstract

Building a general robotic manipulation system capable of performing a wide
variety of tasks in real-world settings is a challenging task. Vision-Language
Models (VLMs) have demonstrated remarkable potential in robotic manipulation
tasks, primarily due to the extensive world knowledge they gain from
large-scale datasets. In this process, Spatial Representations (such as points
representing object positions or vectors representing object orientations) act
as a bridge between VLMs and real-world scene, effectively grounding the
reasoning abilities of VLMs and applying them to specific task scenarios.
However, existing VLM-based robotic approaches often adopt a fixed spatial
representation extraction scheme for various tasks, resulting in insufficient
representational capability or excessive extraction time. In this work, we
introduce T-Rex, a Task-Adaptive Framework for Spatial Representation
Extraction, which dynamically selects the most appropriate spatial
representation extraction scheme for each entity based on specific task
requirements. Our key insight is that task complexity determines the types and
granularity of spatial representations, and Stronger representational
capabilities are typically associated with Higher overall system operation
costs. Through comprehensive experiments in real-world robotic environments, we
show that our approach delivers significant advantages in spatial
understanding, efficiency, and stability without additional training.

## [SEARCH] Summary

...

## [BRAIN] What I Learned

...

## ðŸ”¬ How It Can Be Improved

...

## ðŸ§ª Ideas for Extension

...
