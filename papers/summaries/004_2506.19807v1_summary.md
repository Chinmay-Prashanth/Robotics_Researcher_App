# KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality

**Authors:** Baochang Ren, Shuofei Qiao, Wenhao Yu, Huajun Chen, Ningyu Zhang (Zhejiang University & Tencent AI Seattle Lab)

**Published:** 2025-06-24

**arXiv URL:** https://arxiv.org/abs/2506.19807v1

**Categories:** cs.AI (Artificial Intelligence)

**Pages:** N/A

**PDF Status:** Not encrypted

## üìã Abstract

Large Language Models (LLMs), particularly slow-thinking models, suffer from severe hallucination problems - outputting incorrect content due to an inability to accurately recognize knowledge boundaries during reasoning. While Reinforcement Learning (RL) can enhance complex reasoning abilities, its outcome-oriented rewards often lack factual supervision over the thinking process, further exacerbating hallucinations. This research proposes KnowRL (Knowledge-enhanced RL), which integrates a factuality reward based on knowledge verification into RL training to guide models toward fact-based slow thinking.

## üîç Key Findings

### **1. The Hallucination Problem in Slow-Thinking Models**
- Even large models like DeepSeek-R1-Distill-Qwen-32B achieve only 6.64% accuracy on SimpleQA dataset
- Extended chains of thought amplify initial inaccuracies significantly
- RL-driven by outcome rewards neglects the reasoning process, leading to correct outputs from flawed reasoning

### **2. KnowRL's Effectiveness**
- Significantly improves accuracy on hallucination benchmarks (TruthfulQA, SimpleQA)
- Maintains or improves advanced reasoning skills on datasets like GPQA and AIME 2025
- Achieves 16.23% accuracy on ChineseSimpleQA, demonstrating cross-lingual effectiveness
- Outperforms baseline methods while preserving reasoning capabilities

### **3. Knowledge Boundary Recognition**
- Models learn to better recognize what they know vs. what they don't know
- Reduces fabrication of facts by incorporating external knowledge verification
- Helps models say "I don't know" when appropriate rather than hallucinating

## üß† Technical Methodology

### **Knowledge-Enhanced Reinforcement Learning Framework**
- **Factuality Reward**: Based on evaluating textual support against external knowledge base
- **Inspired by FactScore**: Uses external knowledge verification for reward calculation
- **Dual Optimization**: Cooperatively optimizes reasoning ability and fact-following behavior
- **Knowledge Boundary Learning**: Teaches models to recognize limits of their knowledge

### **Training Process**
1. **Step 1**: Initialize with high-quality factual task dataset
2. **Step 2**: Apply reinforcement learning with combined rewards:
   - Traditional outcome rewards for reasoning
   - Factuality rewards for knowledge adherence
3. **Integration**: External knowledge base provides factual grounding during training

### **Key Innovation**
- Unlike traditional approaches (SFT, RAG, decoding interventions), KnowRL preserves reasoning while reducing hallucinations
- Addresses the "factual supervision gap" in current training paradigms
- Provides explicit factual guidance during the reasoning process

## üí° Research Implications

### **For AI Safety & Reliability**
- Addresses critical hallucination issues in deployment-ready models
- Provides method to maintain reasoning while improving factuality
- Reduces risks of confident but incorrect assertions

### **For Model Training**
- Demonstrates importance of factual supervision during RL training
- Shows feasibility of combining knowledge verification with reasoning optimization
- Provides framework for training more reliable slow-thinking models

## üîß Practical Applications

### **Immediate Use Cases**
1. **Knowledge-Intensive Tasks**: Better performance on factual Q&A systems
2. **Educational AI**: More reliable tutoring and information systems
3. **Professional Assistance**: Reduced hallucination in expert domain applications
4. **Research Tools**: More trustworthy literature review and knowledge synthesis

### **Technical Implementation**
- Can be applied to existing slow-thinking models through additional RL training
- Requires external knowledge base for factuality verification
- Compatible with various model architectures and sizes

## üöÄ Future Research Directions

- **Scaling**: Apply to larger models and broader knowledge domains
- **Efficiency**: Reduce computational overhead of knowledge verification
- **Dynamic Knowledge**: Incorporate updating knowledge bases
- **Multi-modal**: Extend to visual and other modalities beyond text

## üìä Significance

KnowRL represents a significant breakthrough in addressing the hallucination problem that has plagued large language models, particularly in reasoning-intensive applications. By introducing factuality as a first-class concern in reinforcement learning, this work provides a practical path toward more reliable AI systems that can maintain their reasoning capabilities while grounding their outputs in verifiable knowledge.

The framework's success across multiple benchmarks and languages demonstrates its potential for widespread adoption in developing more trustworthy AI assistants and reasoning systems.

## [LIST] Abstract

Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

## [SEARCH] Summary

...

## [BRAIN] What I Learned

...

## üî¨ How It Can Be Improved

...

## üß™ Ideas for Extension

...
