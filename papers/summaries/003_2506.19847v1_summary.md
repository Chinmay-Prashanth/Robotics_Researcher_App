# Orthogonal Finetuning Made Scalable

**Authors:** Zeju Qiu, Weiyang Liu, Adrian Weller, Bernhard SchÃ¶lkopf

**Published:** 2025-06-24

**arXiv URL:** http://arxiv.org/abs/2506.19847v1

**Categories:** cs.LG, cs.AI, cs.CL, cs.CV

**Pages:** 17

**PDF Status:** Not encrypted

## [LIST] Abstract

Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning quantized foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.

## [SEARCH] Summary

...

## [BRAIN] What I Learned

...

## ðŸ”¬ How It Can Be Improved

...

## ðŸ§ª Ideas for Extension

...
