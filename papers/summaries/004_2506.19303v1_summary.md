# Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference

**Authors:** Zexiang Guo, Hengxiang Chen, Xinheng Mai, Qiusang Qiu, Gan Ma, Zhanat Kappassov, Qiang Li, Nutan Chen

**Published:** 2025-06-24

**arXiv URL:** http://arxiv.org/abs/2506.19303v1

**Categories:** cs.RO

**Pages:** 12

**PDF Status:** Not encrypted

## [LIST] Abstract

Inferring physical properties can significantly enhance robotic manipulation
by enabling robots to handle objects safely and efficiently through adaptive
grasping strategies. Previous approaches have typically relied on either
tactile or visual data, limiting their ability to fully capture properties. We
introduce a novel cross-modal perception framework that integrates visual
observations with tactile representations within a multimodal vision-language
model. Our physical reasoning framework, which employs a hierarchical feature
alignment mechanism and a refined prompting strategy, enables our model to make
property-specific predictions that strongly correlate with ground-truth
measurements. Evaluated on 35 diverse objects, our approach outperforms
existing baselines and demonstrates strong zero-shot generalization. Keywords:
tactile perception, visual-tactile fusion, physical property inference,
multimodal integration, robot perception

## [SEARCH] Summary

...

## [BRAIN] What I Learned

...

## ðŸ”¬ How It Can Be Improved

...

## ðŸ§ª Ideas for Extension

...
