# Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning

**Authors:** Guozheng Ma, Lu Li, Zilin Wang, Li Shen, Pierre-Luc Bacon, Dacheng Tao

**Published:** 2025-06-20

**arXiv URL:** http://arxiv.org/abs/2506.17204v1

**Categories:** cs.LG, cs.AI

**Pages:** 21

**PDF Status:** Not encrypted

## ğŸ“‹ Abstract

Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

## ğŸ” Summary

...

## ğŸ§  What I Learned

...

## ğŸ”¬ How It Can Be Improved

...

## ğŸ§ª Ideas for Extension

...
